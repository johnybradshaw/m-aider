# RTX 6000 Ada Generation - Quad GPU (4x 48GB = 192GB VRAM)
# Best for: 32B coder full precision, 72B general purpose, massive context
# Cost: ~$6.00/hour (estimated)

TYPE=g1-gpu-rtx6000-4
REGION=eu-central
IMAGE=linode/ubuntu24.04

# Recommended models for this configuration:
# NOTE: Qwen2.5-Coder-70B does NOT exist. Largest coder is 32B.
# - Qwen/Qwen2.5-Coder-32B-Instruct (FP16/BF16 full precision, recommended)
# - Qwen/Qwen2.5-72B-Instruct (general purpose, not code-specific)
# - meta-llama/Llama-3.1-70B-Instruct (general purpose)

MODEL_ID=Qwen/Qwen2.5-Coder-32B-Instruct
SERVED_MODEL_NAME=coder

# Multi-GPU configuration (REQUIRED for 4 GPUs)
VLLM_TENSOR_PARALLEL_SIZE=4

# Optimized vLLM settings for quad RTX 6000
VLLM_DTYPE=bfloat16
VLLM_MAX_MODEL_LEN=65536
VLLM_GPU_MEMORY_UTILIZATION=0.90
VLLM_MAX_NUM_SEQS=4
# VLLM_EXTRA_ARGS=--enable-prefix-caching

# For general-purpose 72B model:
# MODEL_ID=Qwen/Qwen2.5-72B-Instruct-AWQ
# VLLM_MAX_MODEL_LEN=32768
# VLLM_GPU_MEMORY_UTILIZATION=0.90

# For Llama 3.1 70B:
# MODEL_ID=meta-llama/Llama-3.1-70B-Instruct
# VLLM_DTYPE=bfloat16
# VLLM_MAX_MODEL_LEN=32768
