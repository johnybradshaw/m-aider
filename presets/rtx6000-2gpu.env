# RTX 6000 Ada Generation - Dual GPU (2x 48GB = 96GB VRAM)
# Best for: 32B coder full precision, 72B general purpose models
# Cost: ~$3.00/hour (estimated)

TYPE=g1-gpu-rtx6000-2
REGION=eu-central
IMAGE=linode/ubuntu24.04

# Recommended models for this configuration:
# NOTE: Qwen2.5-Coder-70B does NOT exist. Largest coder is 32B.
# - Qwen/Qwen2.5-Coder-32B-Instruct (FP16 full precision, recommended)
# - Qwen/Qwen2.5-72B-Instruct-AWQ (general purpose 72B, not code-specific)
# - meta-llama/Llama-3.1-70B-Instruct-AWQ (general purpose)

# Default: 32B Coder full precision (best coding model)
MODEL_ID=Qwen/Qwen2.5-Coder-32B-Instruct
SERVED_MODEL_NAME=coder

# Multi-GPU configuration (REQUIRED for 2 GPUs)
VLLM_TENSOR_PARALLEL_SIZE=2

# Optimized vLLM settings for dual RTX 6000 with 32B model
VLLM_DTYPE=bfloat16
VLLM_MAX_MODEL_LEN=32768
VLLM_GPU_MEMORY_UTILIZATION=0.90
VLLM_MAX_NUM_SEQS=2
# VLLM_EXTRA_ARGS=--enable-prefix-caching

# For general purpose 72B model (not code-specific):
# MODEL_ID=Qwen/Qwen2.5-72B-Instruct-AWQ
# SERVED_MODEL_NAME=qwen-72b
# VLLM_MAX_MODEL_LEN=32768
# VLLM_MAX_NUM_SEQS=1

# For Llama 3.1 70B:
# MODEL_ID=meta-llama/Llama-3.1-70B-Instruct-AWQ
# VLLM_MAX_MODEL_LEN=32768
# VLLM_MAX_NUM_SEQS=1
