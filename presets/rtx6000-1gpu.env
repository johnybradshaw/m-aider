# RTX 6000 Ada Generation - Single GPU (48GB VRAM)
# Best for: 32B coder models (quantized), large context
# Cost: ~$1.50/hour

TYPE=g1-gpu-rtx6000-1
REGION=eu-central
IMAGE=linode/ubuntu24.04

# Recommended models for this configuration:
# - Qwen/Qwen2.5-Coder-32B-Instruct-AWQ (best coder, recommended)
# - meta-llama/CodeLlama-34b-Instruct-hf (good alternative)
# - Qwen/Qwen2.5-72B-Instruct-AWQ (general purpose, tight fit)

MODEL_ID=Qwen/Qwen2.5-Coder-32B-Instruct-AWQ
SERVED_MODEL_NAME=coder

# Optimized vLLM settings for RTX 6000
VLLM_MAX_MODEL_LEN=32768
VLLM_GPU_MEMORY_UTILIZATION=0.90
VLLM_MAX_NUM_SEQS=1
# VLLM_EXTRA_ARGS=--enable-prefix-caching

# Multi-GPU settings (leave unset for single GPU)
# VLLM_TENSOR_PARALLEL_SIZE=1

# For general-purpose 72B on single RTX 6000, use conservative settings:
# MODEL_ID=Qwen/Qwen2.5-72B-Instruct-AWQ
# VLLM_MAX_MODEL_LEN=16384
# VLLM_GPU_MEMORY_UTILIZATION=0.88
