# RTX 4000 Ada Generation - Dual GPU (2x 20GB = 40GB VRAM)
# Best for: 32B coder models (quantized), moderate-large context
# Cost: ~$1.04/hour (estimated)

TYPE=g2-gpu-rtx4000a2-s
REGION=de-fra-2
IMAGE=linode/ubuntu24.04

# Recommended models for this configuration:
# - Qwen/Qwen2.5-Coder-32B-Instruct-AWQ (recommended)
# - meta-llama/CodeLlama-34b-Instruct-hf

MODEL_ID=Qwen/Qwen2.5-Coder-32B-Instruct-AWQ
SERVED_MODEL_NAME=coder

# Multi-GPU configuration (REQUIRED for 2 GPUs)
VLLM_TENSOR_PARALLEL_SIZE=2

# Optimized vLLM settings for dual RTX 4000
VLLM_MAX_MODEL_LEN=16384
VLLM_GPU_MEMORY_UTILIZATION=0.90
VLLM_MAX_NUM_SEQS=1
# VLLM_EXTRA_ARGS=--enable-prefix-caching

# Advanced: For larger context, start conservative and increase gradually
# VLLM_MAX_MODEL_LEN=24576
