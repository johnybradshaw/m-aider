# Linode GPU vLLM + Open WebUI + aider Configuration
#
# QUICK START:
#   1. Use a preset: ./presets/use-preset.sh rtx4000-1gpu
#   2. Create .env.secrets: cp .env.secrets.example .env.secrets
#   3. Edit .env.secrets to add your HF token
#   4. Add firewall ID to this file (.env)
#   5. Validate: ./coder validate
#   6. Launch: ./coder up (automatic ready detection + tunnel setup)
#
# See available presets: ./presets/use-preset.sh list

# -----------------------------------------------------------------------------
# REQUIRED: Your Firewall ID
# -----------------------------------------------------------------------------

# Your Linode firewall ID (find with: linode-cli firewalls list)
# This goes in .env (not .env.secrets)
FIREWALL_ID=

# -----------------------------------------------------------------------------
# REQUIRED: HuggingFace Token (goes in .env.secrets, NOT here!)
# -----------------------------------------------------------------------------
#
# Create .env.secrets with your token:
#
#   echo "HUGGING_FACE_HUB_TOKEN=hf_your_token" > .env.secrets
#
# Or use 1Password CLI (most secure):
#
#   echo "HUGGING_FACE_HUB_TOKEN=op://Private/HuggingFace/token" > .env.secrets
#
# .env.secrets is git-ignored for security
# See: .env.secrets.example

# -----------------------------------------------------------------------------
# Infrastructure Configuration
# -----------------------------------------------------------------------------

# Linode region (check availability: linode-cli regions list)
REGION=us-east

# GPU instance type
# Single GPU:
#   TYPE=g2-gpu-rtx4000a1-s   # RTX 4000 Ada (20GB) - ~$0.52/hr
#   TYPE=g1-gpu-rtx6000-1     # RTX 6000 Ada (48GB) - ~$1.50/hr
# Multi-GPU:
#   TYPE=g2-gpu-rtx4000a2-s   # 2x RTX 4000 Ada (40GB) - ~$1.04/hr
#   TYPE=g1-gpu-rtx6000-2     # 2x RTX 6000 Ada (96GB) - ~$3.00/hr
#   TYPE=g1-gpu-rtx6000-4     # 4x RTX 6000 Ada (192GB) - ~$6.00/hr
TYPE=g2-gpu-rtx4000a1-s

# Operating system image (Ubuntu 24.04 LTS recommended)
IMAGE=linode/ubuntu24.04

# -----------------------------------------------------------------------------
# Model Configuration
# -----------------------------------------------------------------------------

# HuggingFace model to serve
# Popular coder models (Qwen2.5-Coder maxes out at 32B):
#   Qwen/Qwen2.5-Coder-7B-Instruct-AWQ
#   Qwen/Qwen2.5-Coder-14B-Instruct-AWQ
#   Qwen/Qwen2.5-Coder-32B-Instruct-AWQ    (largest Qwen coder)
#   deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct-AWQ
#   codellama/CodeLlama-13b-Instruct-hf
# For 70B+ general models (good at coding but not coder-specific):
#   Qwen/Qwen2.5-72B-Instruct-AWQ
MODEL_ID=Qwen/Qwen2.5-Coder-14B-Instruct-AWQ

# Short name for the model (optional, auto-generated if not set)
# This is what you'll use with aider: openai/<SERVED_MODEL_NAME>
SERVED_MODEL_NAME=coder

# -----------------------------------------------------------------------------
# Open WebUI Configuration (Optional)
# -----------------------------------------------------------------------------

# Disable Open WebUI authentication (bypass account creation)
# Set to false to skip login and use the WebUI directly
# OPENWEBUI_AUTH=false

# -----------------------------------------------------------------------------
# vLLM Configuration (Optional - sensible defaults provided)
# -----------------------------------------------------------------------------

# Maximum context length (tokens)
# Larger = more context but uses more VRAM
# Start conservative and increase if stable
# VLLM_MAX_MODEL_LEN=16384

# GPU memory utilization (0.0-1.0)
# Higher = more context headroom but higher OOM risk
# VLLM_GPU_MEMORY_UTILIZATION=0.90

# Maximum number of sequences to process in parallel
# Keep at 1 for coding tasks (maximizes context headroom)
# VLLM_MAX_NUM_SEQS=1

# -----------------------------------------------------------------------------
# Multi-GPU Configuration (REQUIRED for multi-GPU types)
# -----------------------------------------------------------------------------

# Number of GPUs to use for tensor parallelism
# MUST match the GPU count of your TYPE:
#   g2-gpu-rtx4000a1-s or g1-gpu-rtx6000-1: Don't set (defaults to 1)
#   g2-gpu-rtx4000a2-s or g1-gpu-rtx6000-2: Set to 2
#   g1-gpu-rtx6000-4: Set to 4
#
# The launcher will validate this automatically
# VLLM_TENSOR_PARALLEL_SIZE=2

# -----------------------------------------------------------------------------
# Advanced vLLM Settings (Optional)
# -----------------------------------------------------------------------------

# KV cache data type (reduces memory usage)
# Options: fp8_e4m3fn, fp8_e5m2, auto
# VLLM_KV_CACHE_DTYPE=fp8_e4m3fn

# Model weights data type
# Options: half, bfloat16, float16, auto
# VLLM_DTYPE=bfloat16

# Additional vLLM arguments (space-separated)
# Example: --enable-prefix-caching --enforce-eager
# VLLM_EXTRA_ARGS=

# -----------------------------------------------------------------------------
# Presets for Common Configurations
# -----------------------------------------------------------------------------

# Instead of manually configuring, use a preset:
#   ./presets/use-preset.sh list          # See all presets
#   ./presets/use-preset.sh rtx4000-1gpu  # Apply preset
#
# Available presets:
#   rtx4000-1gpu    Single RTX 4000 Ada (7-14B models)
#   rtx4000-2gpu    Dual RTX 4000 Ada (30B models)
#   rtx6000-1gpu    Single RTX 6000 Ada (30-70B quantized)
#   rtx6000-2gpu    Dual RTX 6000 Ada (70B models)
#   rtx6000-4gpu    Quad RTX 6000 Ada (70B full precision)
#
# Presets include optimized vLLM settings and recommended models
